{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ardaolmez/ArdaGpt/blob/main/arda_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I cleaned the data from emojis and I created list for cleaned data. I splited the into lines."
      ],
      "metadata": {
        "id": "aaiGe9nw4ioj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63xRBCnIcVzu",
        "outputId": "77fe59b3-e6a5-47b8-b5bb-40f078b5bcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['01/06/2023, 15:48 - Hatun: Geldik', '01/06/2023, 15:49 - Arda: Sedef ders caliscak m', '01/06/2023, 15:49 - Arda: Nu', '01/06/2023, 15:49 - Arda: Mi', '01/06/2023, 15:49 - Hatun: Evet', '01/06/2023, 15:49 - Arda: Iki', '01/06/2023, 15:49 - Hatun: Ä°ki mi', '01/06/2023, 15:50 - Arda: Okiii', '01/06/2023, 18:18 - Hatun: Ben geldim masaya', '01/06/2023, 21:28 - Hatun: Arda', '01/06/2023, 21:29 - Arda: Aslkiii', '01/06/2023, 21:30 - Hatun: Kutluhan beni sinir etti', '01/06/2023, 21:35 - Arda: Noldu ya', '01/06/2023, 21:35 - Hatun: FayanslarÄ± silmiÅŸ', '01/06/2023, 21:36 - Hatun: Ne zaman geleceksiniz bulaÅŸÄ±klarÄ± yÄ±kayÄ±n', '01/06/2023, 21:36 - Hatun: Bir dolabÄ± silip temizleyelim diyo', '01/06/2023, 21:36 - Hatun: O kadar sinirlendim ki kendisi 2 ay yatÄ±yor hiÃ§bir bok yapmÄ±yor', '01/06/2023, 21:37 - Arda: Asliiiiii bozma ya hiÃ§ gelince silerim ben muhatap olma bjle', '01/06/2023, 21:53 - Hatun: Zaten ben berradayÄ±m ama ÅŸimdi berraya geldim', '01/06/2023, 21:54 - Arda: Ben donuom sikdi', '01/06/2023, 21:54 - Arda: Gelince yazÄ±yÄ±m mi', '01/06/2023, 21:54 - Hatun: Beraber geÃ§elim mi eve', '01/06/2023, 21:55 - Hatun: Gelince ara beni', '01/06/2023, 21:56 - Arda: Olur', '01/06/2023, 22:14 - Hatun: KutluhanÄ± burda bÄ±rakÄ±p kaÃ§abilirim belki', '01/06/2023, 22:15 - Arda: NasÄ±l yani', '01/06/2023, 22:15 - Arda: Kutlu han mi berrada', '01/06/2023, 22:15 - Arda: Ben bindim netroya', '01/06/2023, 22:15 - Hatun: Sen ara beni', '01/06/2023, 22:15 - Hatun: Ben Ã§Ä±karÄ±m', '01/06/2023, 22:15 - Hatun: Evet', '01/06/2023, 22:22 - Hatun: Nerdesin ne zaman kaÃ§Ä±yÄ±', '01/06/2023, 22:22 - Arda: Goethe platztayim', '01/06/2023, 22:38 - Hatun: Gelmek ister misin sormadÄ±m ama', '01/06/2023, 22:39 - Arda: Sen ne yapmak istiosun', '01/06/2023, 22:39 - Hatun: Ben gelirim ya', '01/06/2023, 22:39 - Hatun: Kutluhan burdayken', '01/06/2023, 22:39 - Arda: Oki', '01/06/2023, 22:39 - Hatun: Ajajajajaj', '01/06/2023, 22:39 - Hatun: Maks 5 dk ya Ã§Ä±karÄ±m', '01/06/2023, 22:40 - Hatun: Kutu geliyor ya', '01/06/2023, 22:41 - Arda: Biz yÃ¼rÃ¼rÃ¼z o limelanir', '01/06/2023, 22:41 - Arda: Nsnnsnns', '02/06/2023, 14:48 - Arda: asliiii naptin', '02/06/2023, 15:32 - Hatun: Ardaaa', '02/06/2023, 15:33 - Hatun: 16 da Ã§Ä±karÄ±m', '02/06/2023, 15:33 - Hatun: Sen napÄ±yorsun', '02/06/2023, 15:33 - Arda: video izliom', '02/06/2023, 15:33 - Arda: asliiiiiiiiiiiiiiiiii', '02/06/2023, 15:33 - Arda: napalin sen cikinca', '02/06/2023, 15:33 - Hatun: Kutu', '02/06/2023, 15:33 - Hatun: PlanÄ±m yok', '02/06/2023, 15:34 - Hatun: Ders Ã§alÄ±ÅŸabiliriz istersen', '02/06/2023, 15:34 - Hatun: DÄ±ÅŸarda?', '02/06/2023, 15:34 - Arda: oluuu farkmaz', '02/06/2023, 15:34 - Arda: siiznkier napiyor', '02/06/2023, 15:34 - Hatun: Bilmiyorum', '02/06/2023, 15:34 - Hatun: SorayÄ±m mÄ±', '02/06/2023, 15:35 - Arda: sooor', '02/06/2023, 15:35 - Arda: ben cikip geliiym mi', '02/06/2023, 15:35 - Hatun: Olur sÄ±kÄ±ldÄ±n mÄ±', '02/06/2023, 15:35 - Arda: yoruldum', '02/06/2023, 15:35 - Hatun: Ä°stersen ÅŸimdi Ã§Ä±k yavaÅŸtan', '02/06/2023, 15:36 - Hatun: Cafe bakarÄ±z', '02/06/2023, 15:36 - Arda: ders calicak misin sen', '02/06/2023, 15:36 - Hatun: Neden canÄ±m', '02/06/2023, 15:36 - Hatun: Benim Ã§alÄ±ÅŸmam gerekiyor gibi ya', '02/06/2023, 15:37 - Hatun: Ä°stersen akÅŸama kadar Ã§alÄ±ÅŸalÄ±m akÅŸam dÄ±ÅŸarda plan yapalÄ±m', '02/06/2023, 15:37 - Arda: olur', '02/06/2023, 15:37 - Hatun: Ä°rish pub falan', '02/06/2023, 15:37 - Hatun: Veya yemek yemeye gidelim', '02/06/2023, 15:37 - Hatun: DoÄŸulara da yaz istersen', '02/06/2023, 15:37 - Arda: farkmaz', '02/06/2023, 15:38 - Arda: elif trye dondu', '02/06/2023, 15:38 - Arda: bugun', '02/06/2023, 15:38 - Hatun: Aaaa', '02/06/2023, 15:38 - Arda: berra ve esralarla da bisi yapabiliriz', '02/06/2023, 15:38 - Hatun: YazdÄ±m', '02/06/2023, 15:38 - Hatun: BakalÄ±m yazarlae', '02/06/2023, 15:40 - Hatun: Ä°ÅŸ buldum', '02/06/2023, 15:41 - Arda: nered', '02/06/2023, 15:41 - Arda: e', '02/06/2023, 15:41 - Hatun: 3 tane mail gelmiÅŸ', '02/06/2023, 15:41 - Hatun: Linkedinden', '02/06/2023, 15:41 - Hatun: Onlara baÅŸvuru atÄ±cam', '02/06/2023, 15:41 - Arda: at at', '02/06/2023, 15:41 - Hatun: <Media omitted>', '02/06/2023, 15:42 - Arda: 3.cunu  adi cok havai', '02/06/2023, 15:52 - Arda: Aslii', '02/06/2023, 15:54 - Hatun: Yes', '02/06/2023, 15:54 - Hatun: Foto Ã§ekiyorum', '02/06/2023, 15:55 - Hatun: <Media omitted>', '02/06/2023, 15:55 - Arda: Ben Ã§Ä±ktÄ±m yola', '02/06/2023, 15:55 - Arda: Simdi', '02/06/2023, 16:04 - Hatun: Tamam', '02/06/2023, 16:04 - Hatun: Ben de eÅŸyalarÄ±mÄ± topladÄ±m', '02/06/2023, 16:13 - Arda: B n geldim asli', '02/06/2023, 16:16 - Hatun: Wc ye girip', '02/06/2023, 16:16 - Hatun: Geliyorum', '02/06/2023, 16:23 - Arda: Bum']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "with open('/content/WhatsApp Chat with Ana.txt', 'r', encoding='utf-8') as f0:\n",
        "    text0 = f0.read()\n",
        "with open('/content/WhatsApp Chat with Dogu.txt', 'r', encoding='utf-8') as f1:\n",
        "    text1= f1.read()\n",
        "with open('/content/WhatsApp Chat with Hatun.txt', 'r', encoding='utf-8') as f2:\n",
        "    text2= f2.read()\n",
        "text= text0+text1+text2\n",
        "\n",
        "moj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "moj.sub('',text)\n",
        "textlist=text.splitlines()\n",
        "print(textlist[-100:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I created a new list to add my and my mothers's messages. Because, I had a small size text messages, I added my mom's text also. Characteristics of our messages are similar."
      ],
      "metadata": {
        "id": "z4iE3x6H5lr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0ewwAEjckLJ",
        "outputId": "d16e621f-38b7-4709-b0f9-69a343f88cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86513\n",
            "02/06/2023, 16:16 - Hatun: Geliyorum\n"
          ]
        }
      ],
      "source": [
        "length=len(textlist)\n",
        "print(length)\n",
        "print(textlist[length-2])\n",
        "textlist0=[]\n",
        "for i in range(length):\n",
        "  if \"Arda:\" in textlist[i]:\n",
        "    textlist0.append(textlist[i])\n",
        "  if 'Ana' in textlist[i]:\n",
        "    textlist0.append(textlist[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I cleaned the data from date information and medias."
      ],
      "metadata": {
        "id": "xREcjlzB5957"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XISAg0vjckfV"
      },
      "outputs": [],
      "source": [
        "for i,line in enumerate(textlist0):\n",
        "  if 'Arda:' in line:\n",
        "    textlist0[i]=line[26:]\n",
        "  else:\n",
        "     textlist0[i]=line[25:]\n",
        "\n",
        "for line in textlist0:\n",
        "  if \"<Media omitted>\" in line:\n",
        "    textlist0.remove(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I find all characters which were used in text messages."
      ],
      "metadata": {
        "id": "ha5EJVPR6IAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J3J1JC2ck2s",
        "outputId": "84b01935-a7df-46ca-9c6b-009e1a2b4d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\\]^_abcdefghijklmnopqrstuvwxyz{Â Â£Â²Ã‡Ã–Ã—ÃœÃŸÃ Ã¤Ã§Ã®Ã±Ã¶ÃºÃ¼Ã½ÄÄŸÄ°Ä±ÄºÅ‘ÅÅŸÅ¾â€™â¶â‚¬â˜ºâ™¥âœŒâ¤ï¸ğŸŒ¹ğŸ€ğŸŒğŸğŸğŸ‘€ğŸ’ƒğŸ’ğŸ’ğŸ’«ğŸ˜‚ğŸ˜…ğŸ˜ˆğŸ˜ŠğŸ˜ğŸ˜˜ğŸ˜œğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ™ƒğŸ™ğŸ¤“ğŸ¤£ğŸ¤¨ğŸ¤©ğŸ¤­ğŸ¥°ğŸ¥²ğŸ¥³ğŸ¥´ğŸ¥¹ğŸ¦ğŸ§ğŸ§¿\n",
            "161\n"
          ]
        }
      ],
      "source": [
        "text='\\n'.join(textlist0)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stoi: Map strings to integers. Dictionary for every character assign a number\n",
        "\n",
        "ito: Map integers to strings. Dictionary for every integer get the character\n",
        "\n",
        "(Normally, we use pretrained tokenizer from libraries like huggingface.)\n"
      ],
      "metadata": {
        "id": "BI9CO1DK6Pql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqWLBtraclMc",
        "outputId": "5805bc30-a6d8-417e-8946-209314c64b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37, 65, 78, 2, 65, 82, 84, 73, 75, 2, 87, 72, 65, 84, 83, 85, 80, 2, 43, 2, 66, 85, 82, 65, 68, 65, 78, 2, 75, 85, 76, 76, 65, 78, 112, 67, 65, 77, 1, 36, 69, 78, 2, 65, 78, 78, 69, 78, 1, 54, 65, 77, 65, 77, 68, 85, 82, 2, 65, 78, 78, 69, 1, 35, 78, 78, 78, 69, 1, 48, 65, 80, 73, 89, 79, 82, 83, 85, 78, 1, 37, 65, 78, 73, 77, 1, 35, 82, 68, 65, 67, 73, 77, 1, 47, 73, 83, 83, 69, 68]\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print(encode(text[:100]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EY82Tl3w6mLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we encode our list from my and my mother's messages."
      ],
      "metadata": {
        "id": "bt4DnS637Bjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRaghg0Ncw_d",
        "outputId": "a8f984f5-a8b4-4b29-ae2b-e08db8a4082a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([893878]) torch.int64\n",
            "tensor([37, 65, 78,  2, 65, 82, 84, 73, 75,  2])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we split the data into training and validation lists."
      ],
      "metadata": {
        "id": "qrtH-wLK7H4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPSc3L9dJ4I"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HY8srd4Y7O0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define our hyperparameters and we have a get_batch()\n",
        "to create our batches. Bathces are samples from our data. We use batches to have a faster training, otherwise we use all data in every step. estimate_loss(): calculate losses in every step. Loss estimation no need to take gradients, that is why we use torch.no_grad()\n"
      ],
      "metadata": {
        "id": "ZJb1jr248GDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "batch_size = 8\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "O6JrwpcLsO5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our artitechture. Imagine you're shopping for a new pair of shoes on an e-commerce website. The website's recommendation system uses the key-value-query concept. When you enter your search query (e.g., \"running shoes\"), the system maps this query against a set of keys associated with various shoe products in their database. These keys could include product names, descriptions, brand names, and more.\n",
        "\n",
        "The attention operation, in this context, can be thought of as a retrieval process too.\n",
        "\n",
        "The attention mechanism calculates a weighted average of the values, where each value corresponds to a different shoe product. It does so by assigning weights to each product based on how well they match your query. So, the resulting recommendation you see is akin to retrieving the most relevant shoes from the database based on the weighted values assigned during the attention process. If you remove the constraint of one-hot weighting (where only one product is selected), it becomes a proportional retrieval, with products being retrieved based on their probability weights in the attention calculation."
      ],
      "metadata": {
        "id": "vtHikFRH86Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # masking future tokens\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "kbw86-qttoTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we call head, multiple times. In this way, we have deeper model"
      ],
      "metadata": {
        "id": "0ExB0t9W-pIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NuKT58bAtzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model name is bigram model. It is not related with bigram models directly. Biagram models are the simplest approach for this problem"
      ],
      "metadata": {
        "id": "DjXVcSuH-3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        #we use two embeddings here.\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "1pnK34zruTO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this, section we have training loop. At the end, we print the results."
      ],
      "metadata": {
        "id": "Htdw0FC1_K5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2zYO6Winhoy",
        "outputId": "8f21a02d-bcd7-4e6d-9b3c-ec36bf24bd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.222113 M parameters\n",
            "\tasÄ±zmalim at bakalya\n",
            "BabÄ±m bi dovustuyorum ÅŸarhÄ± ya marahasift vatla ateri. Sen de sikge indisi yatmiyim?\n",
            "htemlikey\n",
            "Tub bak\n",
            "Majil\n",
            "Gorcur\n",
            "Yormaniniyor\n",
            "Yogde\n",
            "Amster benim hazimbin oremaldim ne\n",
            "BÃ¶lÃ¼m seyfz lan\n",
            "Eviyyorum\n",
            "Jsjjejsjjjskjejsjm\n",
            "Buuuuuuum\n",
            "yardi adam gitti\n",
            "Bol tani\n",
            "Sjvta\n",
            "Tent tanin merakla Baka olsun tamardÄ±ÄŸinu daha artik aranamist yok oylek\n",
            "1 dencriidm\n",
            "Bisi adam\n",
            "Diletifylim bu anne\n",
            "Gitmesan gitiorum\n",
            "Ama bir ka nedim\n",
            "Gullsen  balac\n",
            "Sarar fana Ã§ok yanindamm\n",
            "Datioscagijiz i\n",
            "Kcrosin\n",
            "Yok belali youydu\n",
            "Ben balqvarun olaramiz\n",
            "Ne bittius\n",
            "Evettliymesim\n",
            "5Ã¶yje di bugun varayin\n",
            "Kerisi\n",
            "Okeydim\n",
            "Res dece golden vet donuon gelmiyorum\n",
            "J\n",
            "Hayio ne en tr i degkli mi\n",
            "Bunacadikgaldi deyide\n",
            "Kart var\n",
            "Knka bu anne\n",
            "<Media omitted>\n",
            "Da deleter\n",
            "Kelt ka\n",
            "Bibi ahtasap da yapaliodun oni\n",
            "Aem gozdeksin lan\n",
            "50 tak hayatta \n",
            "Rarriliil\n",
            "Kolayim duytum gundekin\n",
            "Hayir anlatÄ±yim\n",
            "Jks bin\n",
            "Prik anfall\n",
            "Rersuller olarima mapli olurdum\n",
            "Ne knka\n",
            "Orda hall\n",
            "Hahah\n",
            "Xmkl\n",
            "Buuuuum\n",
            "Ben yaraki fanemsi\n",
            "Gitti\n",
            "25\n",
            "Genecikz bir iltiom benim\n",
            "8\n",
            "Kilmich\n",
            "Iiling ama ulgÃ¼ne akilmiÅŸ\n",
            "Kisar torya\n",
            "BusaÅŸÄ± konusalar bazÄ±mar yapÄ±rdim\n",
            "Knka wan dedim\n",
            "Hama hic atti\n",
            "Bunlar calisiyor\n",
            "Sjsjsjjjdjjjd\n",
            "Nat and diostunu anne\n",
            "Msal:)\n",
            "O aha daha skiersin\n",
            "Ileyini anne\n",
            "Anj\n",
            "Nasilsole bir sordum\n",
            "Iiidim lail ama\n",
            "<Media omitte Bos olderinlsi\n",
            "#hgiss ortterim\n",
            "Ufyu ya anlarÄ±m\n",
            "Sen\n",
            "KonuÅŸlarlÄ±z ar miyi hapiyor\n",
            "<Media omitted>\n",
            "<Media omitted>\n",
            "Mall S ari**ğŸ¥°ğŸ¥°ğŸ¥°ğŸ¦ğŸ¦ğŸ¦\n",
            "Iiminiz\n",
            "Havadayerim\n",
            "\n",
            "KurtÃ¼rkoc\n",
            "Tamam\n",
            "Gisim sini\n",
            "Suat\n",
            "Jsjsjsjjsjsjskjdjjhsjsjejej\n",
            "Knki gitmesi mic strim\n",
            "yunde degilim Cong tordyili nerede foru seylerek bence\n",
            "Dahvik yapmadi\n",
            "Knka ariada\n",
            "BayadÄ±n mi simdi\n",
            "Ã–m?\n",
            "Kadtra\n",
            "Ã‡ok oglumduji dici biliyor\n",
            "knka oyle\n",
            "Missed voice call\n",
            "Missed voice call.\n",
            "Yok erkeni dersin isli yapiom dedi aslanim\n",
            "aunlara vuuuum\n",
            "Oki sevde\n",
            "Norysp Baba buga gel misin\n",
            "Kotul puseydi\n",
            "Kacildim mi\n",
            "Ahhahahhaha\n",
            "Hih yarin\n",
            "Mogre diyim anne\n",
            "HAhahhaha\n",
            "Variencek fani\n",
            "Ä°girt oillara mi\n",
            "Etmisin benimlende\n",
            "Bunicek aldim\n",
            "Jsjsjdjjdjjdjdjdjdjd\n",
            "Knkak\n",
            "Skjhsjhs\n",
            "Evet ayti\n",
            "OKerin spariv\n",
            "Aninmanma benim onlar\n",
            "Kari\n",
            "G\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        #print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCXlnl24twLNIwBO686QgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}