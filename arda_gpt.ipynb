{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ardaolmez/ArdaGpt/blob/main/arda_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I cleaned the data from emojis and I created list for cleaned data. I splited the into lines."
      ],
      "metadata": {
        "id": "aaiGe9nw4ioj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63xRBCnIcVzu",
        "outputId": "77fe59b3-e6a5-47b8-b5bb-40f078b5bcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['01/06/2023, 15:48 - Hatun: Geldik', '01/06/2023, 15:49 - Arda: Sedef ders caliscak m', '01/06/2023, 15:49 - Arda: Nu', '01/06/2023, 15:49 - Arda: Mi', '01/06/2023, 15:49 - Hatun: Evet', '01/06/2023, 15:49 - Arda: Iki', '01/06/2023, 15:49 - Hatun: İki mi', '01/06/2023, 15:50 - Arda: Okiii', '01/06/2023, 18:18 - Hatun: Ben geldim masaya', '01/06/2023, 21:28 - Hatun: Arda', '01/06/2023, 21:29 - Arda: Aslkiii', '01/06/2023, 21:30 - Hatun: Kutluhan beni sinir etti', '01/06/2023, 21:35 - Arda: Noldu ya', '01/06/2023, 21:35 - Hatun: Fayansları silmiş', '01/06/2023, 21:36 - Hatun: Ne zaman geleceksiniz bulaşıkları yıkayın', '01/06/2023, 21:36 - Hatun: Bir dolabı silip temizleyelim diyo', '01/06/2023, 21:36 - Hatun: O kadar sinirlendim ki kendisi 2 ay yatıyor hiçbir bok yapmıyor', '01/06/2023, 21:37 - Arda: Asliiiiii bozma ya hiç gelince silerim ben muhatap olma bjle', '01/06/2023, 21:53 - Hatun: Zaten ben berradayım ama şimdi berraya geldim', '01/06/2023, 21:54 - Arda: Ben donuom sikdi', '01/06/2023, 21:54 - Arda: Gelince yazıyım mi', '01/06/2023, 21:54 - Hatun: Beraber geçelim mi eve', '01/06/2023, 21:55 - Hatun: Gelince ara beni', '01/06/2023, 21:56 - Arda: Olur', '01/06/2023, 22:14 - Hatun: Kutluhanı burda bırakıp kaçabilirim belki', '01/06/2023, 22:15 - Arda: Nasıl yani', '01/06/2023, 22:15 - Arda: Kutlu han mi berrada', '01/06/2023, 22:15 - Arda: Ben bindim netroya', '01/06/2023, 22:15 - Hatun: Sen ara beni', '01/06/2023, 22:15 - Hatun: Ben çıkarım', '01/06/2023, 22:15 - Hatun: Evet', '01/06/2023, 22:22 - Hatun: Nerdesin ne zaman kaçıyı', '01/06/2023, 22:22 - Arda: Goethe platztayim', '01/06/2023, 22:38 - Hatun: Gelmek ister misin sormadım ama', '01/06/2023, 22:39 - Arda: Sen ne yapmak istiosun', '01/06/2023, 22:39 - Hatun: Ben gelirim ya', '01/06/2023, 22:39 - Hatun: Kutluhan burdayken', '01/06/2023, 22:39 - Arda: Oki', '01/06/2023, 22:39 - Hatun: Ajajajajaj', '01/06/2023, 22:39 - Hatun: Maks 5 dk ya çıkarım', '01/06/2023, 22:40 - Hatun: Kutu geliyor ya', '01/06/2023, 22:41 - Arda: Biz yürürüz o limelanir', '01/06/2023, 22:41 - Arda: Nsnnsnns', '02/06/2023, 14:48 - Arda: asliiii naptin', '02/06/2023, 15:32 - Hatun: Ardaaa', '02/06/2023, 15:33 - Hatun: 16 da çıkarım', '02/06/2023, 15:33 - Hatun: Sen napıyorsun', '02/06/2023, 15:33 - Arda: video izliom', '02/06/2023, 15:33 - Arda: asliiiiiiiiiiiiiiiiii', '02/06/2023, 15:33 - Arda: napalin sen cikinca', '02/06/2023, 15:33 - Hatun: Kutu', '02/06/2023, 15:33 - Hatun: Planım yok', '02/06/2023, 15:34 - Hatun: Ders çalışabiliriz istersen', '02/06/2023, 15:34 - Hatun: Dışarda?', '02/06/2023, 15:34 - Arda: oluuu farkmaz', '02/06/2023, 15:34 - Arda: siiznkier napiyor', '02/06/2023, 15:34 - Hatun: Bilmiyorum', '02/06/2023, 15:34 - Hatun: Sorayım mı', '02/06/2023, 15:35 - Arda: sooor', '02/06/2023, 15:35 - Arda: ben cikip geliiym mi', '02/06/2023, 15:35 - Hatun: Olur sıkıldın mı', '02/06/2023, 15:35 - Arda: yoruldum', '02/06/2023, 15:35 - Hatun: İstersen şimdi çık yavaştan', '02/06/2023, 15:36 - Hatun: Cafe bakarız', '02/06/2023, 15:36 - Arda: ders calicak misin sen', '02/06/2023, 15:36 - Hatun: Neden canım', '02/06/2023, 15:36 - Hatun: Benim çalışmam gerekiyor gibi ya', '02/06/2023, 15:37 - Hatun: İstersen akşama kadar çalışalım akşam dışarda plan yapalım', '02/06/2023, 15:37 - Arda: olur', '02/06/2023, 15:37 - Hatun: İrish pub falan', '02/06/2023, 15:37 - Hatun: Veya yemek yemeye gidelim', '02/06/2023, 15:37 - Hatun: Doğulara da yaz istersen', '02/06/2023, 15:37 - Arda: farkmaz', '02/06/2023, 15:38 - Arda: elif trye dondu', '02/06/2023, 15:38 - Arda: bugun', '02/06/2023, 15:38 - Hatun: Aaaa', '02/06/2023, 15:38 - Arda: berra ve esralarla da bisi yapabiliriz', '02/06/2023, 15:38 - Hatun: Yazdım', '02/06/2023, 15:38 - Hatun: Bakalım yazarlae', '02/06/2023, 15:40 - Hatun: İş buldum', '02/06/2023, 15:41 - Arda: nered', '02/06/2023, 15:41 - Arda: e', '02/06/2023, 15:41 - Hatun: 3 tane mail gelmiş', '02/06/2023, 15:41 - Hatun: Linkedinden', '02/06/2023, 15:41 - Hatun: Onlara başvuru atıcam', '02/06/2023, 15:41 - Arda: at at', '02/06/2023, 15:41 - Hatun: <Media omitted>', '02/06/2023, 15:42 - Arda: 3.cunu  adi cok havai', '02/06/2023, 15:52 - Arda: Aslii', '02/06/2023, 15:54 - Hatun: Yes', '02/06/2023, 15:54 - Hatun: Foto çekiyorum', '02/06/2023, 15:55 - Hatun: <Media omitted>', '02/06/2023, 15:55 - Arda: Ben çıktım yola', '02/06/2023, 15:55 - Arda: Simdi', '02/06/2023, 16:04 - Hatun: Tamam', '02/06/2023, 16:04 - Hatun: Ben de eşyalarımı topladım', '02/06/2023, 16:13 - Arda: B n geldim asli', '02/06/2023, 16:16 - Hatun: Wc ye girip', '02/06/2023, 16:16 - Hatun: Geliyorum', '02/06/2023, 16:23 - Arda: Bum']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "with open('/content/WhatsApp Chat with Ana.txt', 'r', encoding='utf-8') as f0:\n",
        "    text0 = f0.read()\n",
        "with open('/content/WhatsApp Chat with Dogu.txt', 'r', encoding='utf-8') as f1:\n",
        "    text1= f1.read()\n",
        "with open('/content/WhatsApp Chat with Hatun.txt', 'r', encoding='utf-8') as f2:\n",
        "    text2= f2.read()\n",
        "text= text0+text1+text2\n",
        "\n",
        "moj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "moj.sub('',text)\n",
        "textlist=text.splitlines()\n",
        "print(textlist[-100:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I created a new list to add my and my mothers's messages. Because, I had a small size text messages, I added my mom's text also. Characteristics of our messages are similar."
      ],
      "metadata": {
        "id": "z4iE3x6H5lr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0ewwAEjckLJ",
        "outputId": "d16e621f-38b7-4709-b0f9-69a343f88cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86513\n",
            "02/06/2023, 16:16 - Hatun: Geliyorum\n"
          ]
        }
      ],
      "source": [
        "length=len(textlist)\n",
        "print(length)\n",
        "print(textlist[length-2])\n",
        "textlist0=[]\n",
        "for i in range(length):\n",
        "  if \"Arda:\" in textlist[i]:\n",
        "    textlist0.append(textlist[i])\n",
        "  if 'Ana' in textlist[i]:\n",
        "    textlist0.append(textlist[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I cleaned the data from date information and medias."
      ],
      "metadata": {
        "id": "xREcjlzB5957"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XISAg0vjckfV"
      },
      "outputs": [],
      "source": [
        "for i,line in enumerate(textlist0):\n",
        "  if 'Arda:' in line:\n",
        "    textlist0[i]=line[26:]\n",
        "  else:\n",
        "     textlist0[i]=line[25:]\n",
        "\n",
        "for line in textlist0:\n",
        "  if \"<Media omitted>\" in line:\n",
        "    textlist0.remove(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I find all characters which were used in text messages."
      ],
      "metadata": {
        "id": "ha5EJVPR6IAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J3J1JC2ck2s",
        "outputId": "84b01935-a7df-46ca-9c6b-009e1a2b4d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\\]^_abcdefghijklmnopqrstuvwxyz{ £²ÇÖ×ÜßàäçîñöúüýďğİıĺőŞşž’⁶€☺♥✌❤️🌹🍀🍌🍎🍏👀💃💐💝💫😂😅😈😊😎😘😜😡😢😭🙃🙏🤓🤣🤨🤩🤭🥰🥲🥳🥴🥹🦍🧐🧿\n",
            "161\n"
          ]
        }
      ],
      "source": [
        "text='\\n'.join(textlist0)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stoi: Map strings to integers. Dictionary for every character assign a number\n",
        "\n",
        "ito: Map integers to strings. Dictionary for every integer get the character\n",
        "\n",
        "(Normally, we use pretrained tokenizer from libraries like huggingface.)\n"
      ],
      "metadata": {
        "id": "BI9CO1DK6Pql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqWLBtraclMc",
        "outputId": "5805bc30-a6d8-417e-8946-209314c64b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37, 65, 78, 2, 65, 82, 84, 73, 75, 2, 87, 72, 65, 84, 83, 85, 80, 2, 43, 2, 66, 85, 82, 65, 68, 65, 78, 2, 75, 85, 76, 76, 65, 78, 112, 67, 65, 77, 1, 36, 69, 78, 2, 65, 78, 78, 69, 78, 1, 54, 65, 77, 65, 77, 68, 85, 82, 2, 65, 78, 78, 69, 1, 35, 78, 78, 78, 69, 1, 48, 65, 80, 73, 89, 79, 82, 83, 85, 78, 1, 37, 65, 78, 73, 77, 1, 35, 82, 68, 65, 67, 73, 77, 1, 47, 73, 83, 83, 69, 68]\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print(encode(text[:100]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EY82Tl3w6mLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we encode our list from my and my mother's messages."
      ],
      "metadata": {
        "id": "bt4DnS637Bjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRaghg0Ncw_d",
        "outputId": "a8f984f5-a8b4-4b29-ae2b-e08db8a4082a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([893878]) torch.int64\n",
            "tensor([37, 65, 78,  2, 65, 82, 84, 73, 75,  2])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we split the data into training and validation lists."
      ],
      "metadata": {
        "id": "qrtH-wLK7H4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPSc3L9dJ4I"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HY8srd4Y7O0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define our hyperparameters and we have a get_batch()\n",
        "to create our batches. Bathces are samples from our data. We use batches to have a faster training, otherwise we use all data in every step. estimate_loss(): calculate losses in every step. Loss estimation no need to take gradients, that is why we use torch.no_grad()\n"
      ],
      "metadata": {
        "id": "ZJb1jr248GDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "batch_size = 8\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "O6JrwpcLsO5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our artitechture. Imagine you're shopping for a new pair of shoes on an e-commerce website. The website's recommendation system uses the key-value-query concept. When you enter your search query (e.g., \"running shoes\"), the system maps this query against a set of keys associated with various shoe products in their database. These keys could include product names, descriptions, brand names, and more.\n",
        "\n",
        "The attention operation, in this context, can be thought of as a retrieval process too.\n",
        "\n",
        "The attention mechanism calculates a weighted average of the values, where each value corresponds to a different shoe product. It does so by assigning weights to each product based on how well they match your query. So, the resulting recommendation you see is akin to retrieving the most relevant shoes from the database based on the weighted values assigned during the attention process. If you remove the constraint of one-hot weighting (where only one product is selected), it becomes a proportional retrieval, with products being retrieved based on their probability weights in the attention calculation."
      ],
      "metadata": {
        "id": "vtHikFRH86Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # masking future tokens\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "kbw86-qttoTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we call head, multiple times. In this way, we have deeper model"
      ],
      "metadata": {
        "id": "0ExB0t9W-pIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NuKT58bAtzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model name is bigram model. It is not related with bigram models directly. Biagram models are the simplest approach for this problem"
      ],
      "metadata": {
        "id": "DjXVcSuH-3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        #we use two embeddings here.\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "1pnK34zruTO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this, section we have training loop. At the end, we print the results."
      ],
      "metadata": {
        "id": "Htdw0FC1_K5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2zYO6Winhoy",
        "outputId": "8f21a02d-bcd7-4e6d-9b3c-ec36bf24bd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.222113 M parameters\n",
            "\tasızmalim at bakalya\n",
            "Babım bi dovustuyorum şarhı ya marahasift vatla ateri. Sen de sikge indisi yatmiyim?\n",
            "htemlikey\n",
            "Tub bak\n",
            "Majil\n",
            "Gorcur\n",
            "Yormaniniyor\n",
            "Yogde\n",
            "Amster benim hazimbin oremaldim ne\n",
            "Bölüm seyfz lan\n",
            "Eviyyorum\n",
            "Jsjjejsjjjskjejsjm\n",
            "Buuuuuuum\n",
            "yardi adam gitti\n",
            "Bol tani\n",
            "Sjvta\n",
            "Tent tanin merakla Baka olsun tamardığinu daha artik aranamist yok oylek\n",
            "1 dencriidm\n",
            "Bisi adam\n",
            "Diletifylim bu anne\n",
            "Gitmesan gitiorum\n",
            "Ama bir ka nedim\n",
            "Gullsen  balac\n",
            "Sarar fana çok yanindamm\n",
            "Datioscagijiz i\n",
            "Kcrosin\n",
            "Yok belali youydu\n",
            "Ben balqvarun olaramiz\n",
            "Ne bittius\n",
            "Evettliymesim\n",
            "5öyje di bugun varayin\n",
            "Kerisi\n",
            "Okeydim\n",
            "Res dece golden vet donuon gelmiyorum\n",
            "J\n",
            "Hayio ne en tr i degkli mi\n",
            "Bunacadikgaldi deyide\n",
            "Kart var\n",
            "Knka bu anne\n",
            "<Media omitted>\n",
            "Da deleter\n",
            "Kelt ka\n",
            "Bibi ahtasap da yapaliodun oni\n",
            "Aem gozdeksin lan\n",
            "50 tak hayatta \n",
            "Rarriliil\n",
            "Kolayim duytum gundekin\n",
            "Hayir anlatıyim\n",
            "Jks bin\n",
            "Prik anfall\n",
            "Rersuller olarima mapli olurdum\n",
            "Ne knka\n",
            "Orda hall\n",
            "Hahah\n",
            "Xmkl\n",
            "Buuuuum\n",
            "Ben yaraki fanemsi\n",
            "Gitti\n",
            "25\n",
            "Genecikz bir iltiom benim\n",
            "8\n",
            "Kilmich\n",
            "Iiling ama ulgüne akilmiş\n",
            "Kisar torya\n",
            "Busaşı konusalar bazımar yapırdim\n",
            "Knka wan dedim\n",
            "Hama hic atti\n",
            "Bunlar calisiyor\n",
            "Sjsjsjjjdjjjd\n",
            "Nat and diostunu anne\n",
            "Msal:)\n",
            "O aha daha skiersin\n",
            "Ileyini anne\n",
            "Anj\n",
            "Nasilsole bir sordum\n",
            "Iiidim lail ama\n",
            "<Media omitte Bos olderinlsi\n",
            "#hgiss ortterim\n",
            "Ufyu ya anlarım\n",
            "Sen\n",
            "Konuşlarlız ar miyi hapiyor\n",
            "<Media omitted>\n",
            "<Media omitted>\n",
            "Mall S ari**🥰🥰🥰🦍🦍🦍\n",
            "Iiminiz\n",
            "Havadayerim\n",
            "\n",
            "Kurtürkoc\n",
            "Tamam\n",
            "Gisim sini\n",
            "Suat\n",
            "Jsjsjsjjsjsjskjdjjhsjsjejej\n",
            "Knki gitmesi mic strim\n",
            "yunde degilim Cong tordyili nerede foru seylerek bence\n",
            "Dahvik yapmadi\n",
            "Knka ariada\n",
            "Bayadın mi simdi\n",
            "Öm?\n",
            "Kadtra\n",
            "Çok oglumduji dici biliyor\n",
            "knka oyle\n",
            "Missed voice call\n",
            "Missed voice call.\n",
            "Yok erkeni dersin isli yapiom dedi aslanim\n",
            "aunlara vuuuum\n",
            "Oki sevde\n",
            "Norysp Baba buga gel misin\n",
            "Kotul puseydi\n",
            "Kacildim mi\n",
            "Ahhahahhaha\n",
            "Hih yarin\n",
            "Mogre diyim anne\n",
            "HAhahhaha\n",
            "Variencek fani\n",
            "İgirt oillara mi\n",
            "Etmisin benimlende\n",
            "Bunicek aldim\n",
            "Jsjsjdjjdjjdjdjdjdjd\n",
            "Knkak\n",
            "Skjhsjhs\n",
            "Evet ayti\n",
            "OKerin spariv\n",
            "Aninmanma benim onlar\n",
            "Kari\n",
            "G\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        #print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCXlnl24twLNIwBO686QgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}